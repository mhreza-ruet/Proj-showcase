{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a590bd5f",
   "metadata": {},
   "source": [
    "### User input & data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cfaaa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:114: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:115: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:114: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:115: SyntaxWarning: invalid escape sequence '\\e'\n",
      "/tmp/ipykernel_7838/2627130355.py:114: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"Write section headings exactly as \\section{Introduction}, \\section{Methods}, \\section{Results}, \\section{Discussion}.\\n\"\n",
      "/tmp/ipykernel_7838/2627130355.py:115: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  \"Do not use \\emph, \\textbf, or extra braces inside \\section or \\subsection\"\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np, pandas as pd, re, json, os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import subprocess, tempfile, shutil, pathlib\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "load_dotenv() \n",
    "experiment = {\n",
    "    \"context\": (\n",
    "        # \"I am a graduate student writing an initial report on phosphate adsorption in saline soils \"\n",
    "        # \"(Songnen Plain). I ran batch adsorption tests by varying the initial phosphate concentration (C0), \"\n",
    "        # \"solution volume (V), sorbent mass (m), pH, temperature (T), and background salinity (NaCl). \"\n",
    "        # \"At equilibrium I measured the liquid-phase concentration (Ce) and computed adsorption capacity q.\"\n",
    "        \"Record a concise observations-style report about batch adsorption of phosphate in saline soils \"\n",
    "        \"(Songnen Plain). The study varies initial concentration, volume, sorbent mass, pH, temperature, \"\n",
    "        \"and NaCl salinity, measures the equilibrium liquid concentration, and summarizes observed adsorption capacity \"\n",
    "        \"and removal. Write as if reporting what was done and what was seenâ€”no derivations.\"\n",
    "    ),\n",
    "    \"motivation\": (\n",
    "        # \"Our near-term goal is to derive and validate the phosphate adsorption capacity equation \"\n",
    "        # \"q = ((C0 - Ce) * V) / m and describe how salinity, pH, and temperature influence removal.\"\n",
    "        \"Provide a clear record of the experimental conditions and numerical outcomes (counts, ranges, means/SDs, \"\n",
    "        \"optional 95% CIs). Emphasize trends seen in the data (effects of salinity, pH, temperature) and note any \"\n",
    "        \"limitations. Keep it practical, like a lab note intended for a follow-up meeting.\"\n",
    "    ),\n",
    "\n",
    "    \"NUM_DATASETS\": 1,\n",
    "    \"N_OBS\": 12,\n",
    "\n",
    "    # Inputs: choose ranges typical of batch adsorption tests in soils\n",
    "    \"inputs\": {\n",
    "        \"C0_(mg/L)\"     : (\"mg/L\", \"uniform\", [5.0, 150.0]),   # initial phosphate concentration\n",
    "        \"V_(mL)\"        : (\"mL\",   \"uniform\", [24.8, 25.2]),   # ~25 mL batch volume\n",
    "        \"m_(g)\"         : (\"g\",    \"uniform\", [1.20, 1.30]),   # ~1.25 g sorbent mass\n",
    "        \"pH\"            : (\"\",     \"uniform\", [4.0, 9.0]),     # acidic to alkaline\n",
    "        \"T_(K)\"         : (\"K\",    \"uniform\", [288.0, 308.0]), # 15â€“35 Â°C\n",
    "        \"salinity_(M)\"  : (\"M\",    \"uniform\", [0.01, 0.20])    # background ionic strength proxy\n",
    "    },\n",
    "\n",
    "    # Outputs: Ce is generated via a bounded removal fraction f(pH, T, salinity), then q_e from capacity eqn\n",
    "    \"outputs\": {\n",
    "        # Equilibrium concentration in solution (Ce)\n",
    "        # removal_frac = clip( 0.25 + 0.40*sal_term + 0.15*pH_term + 0.10*temp_term, 0.05, 0.90 )\n",
    "        # where sal_term = (salinity-0.01)/0.19, pH_term = (clip(pH,4,9)-4)/5, temp_term = (T-298)/10\n",
    "        \"Ce_(mg/L)\": (\n",
    "            \"mg/L\",\n",
    "            \"C0_(mg/L) * (1.0 - np.minimum(0.90, np.maximum(0.05, \"\n",
    "            \"0.25 + 0.40*((salinity_(M)-0.01)/0.19) + \"\n",
    "            \"0.15*((np.minimum(np.maximum(pH,4.0),9.0)-4.0)/5.0) + \"\n",
    "            \"0.10*((T_(K)-298.0)/10.0) )))\",\n",
    "            (\"gaussian\", [0.0, 0.25])  # light measurement noise on Ce (mg/L)\n",
    "        ),\n",
    "\n",
    "        # Equilibrium adsorption capacity q_e = ((C0 - Ce) * V) / m\n",
    "        # Convert mL -> L by dividing V by 1000, units end up in mg/g\n",
    "        \"q_e_(mg/g)\": (\n",
    "            \"mg/g\",\n",
    "            \"((C0_(mg/L) - Ce_(mg/L)) * (V_(mL) / 1000.0)) / m_(g)\",\n",
    "            (\"gaussian\", [0.0, 0.02])  # tiny noise on computed capacity\n",
    "        ),\n",
    "\n",
    "        # Percent removal = 100 * (C0 - Ce)/C0\n",
    "        \"Removal_(%)\": (\n",
    "            \"%\",\n",
    "            \"100.0 * (C0_(mg/L) - Ce_(mg/L)) / C0_(mg/L)\",\n",
    "            (\"gaussian\", [0.0, 0.2])\n",
    "        )\n",
    "    },\n",
    "\n",
    "    \"paper_words\": 1000,\n",
    "    \"wrap_words\": 20,\n",
    "    \"model_id\": \"gpt-4.1\",\n",
    "    \"tok_paper\": 3500,\n",
    "    \"tok_titleabs\": 500,\n",
    "    \"out_dir\": \"Output/Phosphate_Songnen_run\"\n",
    "}\n",
    "\n",
    "# experiment[\"context\"] = (\n",
    "#     \"I am a graduate student writing an initial report on phosphate adsorption in saline soils \"\n",
    "#     \"(Songnen Plain). I ran batch adsorption tests by varying the initial phosphate concentration (C0), \"\n",
    "#     \"solution volume (V), sorbent mass (m), pH, temperature (T), and background salinity (NaCl). \"\n",
    "#     \"At equilibrium I measured the liquid-phase concentration (Ce) and computed adsorption capacity q.\\n\\n\"\n",
    "#     \"CONSTRAINTS FOR THE MODEL (do not include this paragraph in the paper): \"\n",
    "#     \"Write a LaTeX paper with sections (Introduction, Methods, Results, Discussion). \"\n",
    "#     \"Never mention dataset generation, scripts, prompts, ground-truth formulas, noise, code, or language models. \"\n",
    "#     \"Use math for symbols (e.g., $q_e$, $C_0$, $C_e$). Write percent as '\\\\%' (do NOT wrap percent in \\\\si{...}). \"\n",
    "#     \"Use siunitx for units like \\\\si{mg\\\\per L}, \\\\si{mg\\\\per g}, \\\\si{mL}, \\\\si{K}. \"\n",
    "#     \"Refer to the adsorption capacity equation as $q_e=\\\\frac{(C_0-C_e)V}{m}$ and state that V is converted from mL to L; \"\n",
    "#     \"report $q_e$ in \\\\si{mg\\\\per g}. Use the actual sample size N=\"\n",
    "#     f\"{experiment['N_OBS']}\"\n",
    "#     \" when referring to counts. Do not list variable-generation details or any mention of 'inputs x_i' or 'responses y_k'. \"\n",
    "#     \"Avoid boilerplate like 'as an AI'. Use single backslashes for LaTeX macros (e.g., \\\\textbf, not \\\\\\\\textbf).\"\n",
    "# )\n",
    "\n",
    "# experiment[\"motivation\"] = (\n",
    "#     \"Our near-term goal is to derive and validate the phosphate adsorption capacity equation \"\n",
    "#     \"$q_e=\\\\frac{(C_0-C_e)V}{m}$ across the tested ranges and describe how salinity, pH, and temperature influence removal. \"\n",
    "#     \"Report concise summary statistics (means/SDs, optionally 95\\\\% CIs) and give a brief quantitative interpretation. \"\n",
    "#     \"If appropriate, you may mention isotherm models (Langmuir/Freundlich) qualitatively, but do not expose any hidden data-generation formulas.\"\n",
    "# )\n",
    "experiment[\"constraints\"] = (\n",
    "    \"[INSTRUCTIONSâ€”DO NOT INCLUDE THIS PARAGRAPH IN THE PAPER]\\n\"\n",
    "    \"Style: observational log. Focus on what was done and what was observed.\\n\"\n",
    "    \"Do NOT print any equations or generating formulas anywhere (no '=' in math). Prefer plain language over symbols; \"\n",
    "    \"if a symbol is unavoidable, keep it inline (e.g., C0, Ce) but never show a formula.\\n\"\n",
    "    \"Do NOT include an 'Inputs'/'Outputs' bullet list or any bullet list of variable definitions.\\n\"\n",
    "    \"Do NOT mention scripts, prompts, templates, ground-truth, noise, or code tokens (np., Uniform(â€¦), clip(â€¦), min/max).\\n\"\n",
    "    f\"When giving counts, use the actual sample size N={experiment['N_OBS']}. Report units with siunitx \"\n",
    "    \"(\\\\si{{mg\\\\per L}}, \\\\si{{mg\\\\per g}}, \\\\si{{mL}}, \\\\si{{K}}). Percent must be written as '\\\\%' or \"\n",
    "    \"\\\\SI{{..}}{{\\\\percent}} (never '\\\\\\\\%' or '\\\\si{{%}}').\\n\"\n",
    "    \"Sections required: Introduction, Methods, Results, Discussion. Keep each section compact and grounded in the data.\\n\"\n",
    "    \"Write section headings exactly as \\section{Introduction}, \\section{Methods}, \\section{Results}, \\section{Discussion}.\\n\"\n",
    "    \"Do not use \\emph, \\textbf, or extra braces inside \\section or \\subsection\"\n",
    "    \n",
    ")\n",
    "\n",
    "# experiment[\"motivation\"] = (\n",
    "#     \"We aim to quantify phosphate retention in saline soils from the Songnen Plain and validate \"\n",
    "#     \"the mass-balance capacity equation across realistic conditions (C0, V, m, pH, T, NaCl). \"\n",
    "#     \"We summarize how salinity, pH, and temperature modulate removal efficiency and qe, providing \"\n",
    "#     \"a baseline template for later isotherm/kinetics fits and treatment comparisons.\"\n",
    "# )\n",
    "\n",
    "client = OpenAI()\n",
    "rng    = np.random.default_rng(2025)\n",
    "OUT    = Path(experiment[\"out_dir\"]); OUT.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "dists  = {\n",
    "    \"uniform\": lambda a,b,n: rng.uniform(a,b,n),\n",
    "    \"normal\" : lambda m,s,n: rng.normal(m,s,n),\n",
    "    \"int\"    : lambda a,b,n: rng.integers(a,b+1,n),\n",
    "}\n",
    "\n",
    "safe_pat = re.compile(r'[^0-9a-zA-Z_]')\n",
    "def safe(col): return safe_pat.sub('_', col)\n",
    "\n",
    "BAN_PATTERNS = re.compile(\n",
    "    r\"(constraints of this platform|not feasible|token limit|as an ai|i cannot|i can't|\"\n",
    "    r\"model cannot|chatgpt|this interface|outline only)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "def strip_meta(text: str) -> str:\n",
    "    return \"\\n\".join(ln for ln in text.splitlines() if not BAN_PATTERNS.search(ln)).strip()\n",
    "\n",
    "def sample_inputs(n):\n",
    "    return pd.DataFrame({\n",
    "        col: dists[dist](*params, n)\n",
    "        for col, (_, dist, params) in experiment[\"inputs\"].items()\n",
    "    })\n",
    "\n",
    "def add_outputs(df):\n",
    "    for name, (_, formula, noise) in experiment[\"outputs\"].items():\n",
    "        expr = formula.replace('^','**')\n",
    "        env  = {\"np\": np}\n",
    "        for orig in df.columns:\n",
    "            token     = safe(orig)\n",
    "            env[token] = df[orig]\n",
    "            expr      = expr.replace(orig, token)\n",
    "        y = eval(expr, env)\n",
    "        if noise[0] == \"gaussian\":\n",
    "            mu, sd = noise[1]\n",
    "            y += rng.normal(mu, sd, len(df))\n",
    "        df[name] = y\n",
    "    return df.round(6)\n",
    "\n",
    "def stats_json(df):\n",
    "    return json.dumps(\n",
    "        df.describe().T[[\"mean\",\"std\",\"min\",\"max\"]].round(4).to_dict(), indent=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec0fdcb",
   "metadata": {},
   "source": [
    "### LaTeX formatting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2f29a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/tmp/ipykernel_7838/1015078613.py:8: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  repl = {'\\\\': r'\\textbackslash{}','{': r'\\{','}': r'\\}','\\$': r'\\$','&': r'\\&',\n"
     ]
    }
   ],
   "source": [
    "def markdown_emphasis_to_tex(text: str) -> str:\n",
    "    text = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"\\\\textbf{\\1}\", text, flags=re.S)\n",
    "    text = re.sub(r\"(?<!\\\\)\\*(?!\\s)(.+?)(?<!\\s)\\*\", r\"\\\\emph{\\1}\", text, flags=re.S)\n",
    "    return text\n",
    "\n",
    "def latex_escape(s: str) -> str:\n",
    "    if s is None: return \"\"\n",
    "    repl = {'\\\\': r'\\textbackslash{}','{': r'\\{','}': r'\\}','\\$': r'\\$','&': r'\\&',\n",
    "            '%': r'\\%','#': r'\\#','_': r'\\_','^': r'\\^{}','~': r'\\~{}'}\n",
    "    return \"\".join(repl.get(ch, ch) for ch in s)\n",
    "\n",
    "_unit_caret_pat   = re.compile(r'([A-Za-z])(\\d+)')\n",
    "_unit_negexp_pat  = re.compile(r'([A-Za-z])-(\\d+)')\n",
    "def _unit_to_si(u: str) -> str:\n",
    "    u = u.strip()\n",
    "    u = _unit_caret_pat.sub(r'\\1^\\2', u)\n",
    "    u = _unit_negexp_pat.sub(r'\\1^{-\\2}', u)\n",
    "    u = u.replace(\"/\", r\"\\per \")\n",
    "    return r\"\\si{\" + u + \"}\"\n",
    "\n",
    "def _format_header_with_units(colname: str) -> str:\n",
    "    pretty = colname.replace(\"_\", \" \")\n",
    "    m = re.match(r\"^(.*)\\((.*)\\)\\s*$\", pretty)\n",
    "    if m:\n",
    "        return f\"{latex_escape(m.group(1).strip())} ({_unit_to_si(m.group(2).strip())})\"\n",
    "    return latex_escape(pretty)\n",
    "\n",
    "def df_to_latex_table(df: pd.DataFrame,\n",
    "                      caption=\"Generated Data\",\n",
    "                      label=\"tab:data\",\n",
    "                      floatfmt=6) -> str:\n",
    "    df2 = df.copy()\n",
    "    for c in df2.select_dtypes(include=\"float\"):\n",
    "        df2[c] = df2[c].round(floatfmt)\n",
    "    cols_fmt = [_format_header_with_units(c) for c in df2.columns]\n",
    "    col_spec = \"c\" * len(cols_fmt)\n",
    "    header = \" & \".join(cols_fmt) + r\" \\\\ \\midrule\"\n",
    "    rows = \"\\n\".join(\" & \".join(map(str, r)) + r\" \\\\\" for r in df2.values)\n",
    "    return (\n",
    "        \"\\\\begin{table}[htbp]\\n\"\n",
    "        \"\\\\centering\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\"\n",
    "        f\"\\\\label{{{label}}}\\n\"\n",
    "        \"\\\\small\\n\"\n",
    "        f\"\\\\begin{{tabular}}{{{col_spec}}}\\n\"\n",
    "        \"\\\\toprule\\n\"\n",
    "        f\"{header}\\n\"\n",
    "        f\"{rows}\\n\"\n",
    "        \"\\\\bottomrule\\n\"\n",
    "        \"\\\\end{tabular}\\n\"\n",
    "        \"\\\\end{table}\\n\"\n",
    "    )\n",
    "\n",
    "def _dist_to_tex(dist: str, params) -> str:\n",
    "    d = dist.lower()\n",
    "    if d == \"uniform\": a,b = params;  return rf\"\\mathrm{{Uniform}}({a}, {b})\"\n",
    "    if d == \"normal\":  mu,sd = params; return rf\"\\mathcal{{N}}({mu},\\,{sd})\"\n",
    "    if d == \"int\":     a,b = params;  return rf\"\\mathrm{{DiscreteUniform}}({a}, {b})\"\n",
    "    return latex_escape(f\"{dist}({params})\")\n",
    "\n",
    "def _formula_to_latex(formula: str, input_names: list, output_names: list):\n",
    "    in_syms  = {name: rf\"x_{i}\" for i, name in enumerate(input_names, 1)}\n",
    "    out_syms = {name: rf\"y_{j}\" for j, name in enumerate(output_names, 1)}\n",
    "    f = formula.replace(\"np.pi\", r\"\\pi\").replace(\"**\", \"^\").replace(\"*\", r\" \\cdot \")\n",
    "    for name, sym in in_syms.items():  f = f.replace(name, sym)\n",
    "    for name, sym in out_syms.items(): f = f.replace(name, sym)\n",
    "    return f, in_syms, out_syms\n",
    "\n",
    "def _topic_phrase_from_context(ctx: str) -> str:\n",
    "    if not ctx:\n",
    "        return \"the study topic\"\n",
    "    s = ctx.strip()\n",
    "    s = re.split(r'[.!?\\n]', s, 1)[0]\n",
    "    s = re.sub(r\",?\\s*so\\s+(?:I|we)\\b.*$\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"^\\s*(?:I|we)\\s+(?:am|are)\\s+\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"^\\s*(?:this\\s+(?:paper|study)\\s+)?(?:aims?|seeks?|tries?)\\s+to\\s+\", \"\", s, flags=re.I)\n",
    "    s = re.sub(r\"^estimating\\b\",   \"estimation of\",  s, flags=re.I)\n",
    "    s = re.sub(r\"^predicting\\b\",   \"prediction of\",  s, flags=re.I)\n",
    "    s = re.sub(r\"^measuring\\b\",    \"measurement of\", s, flags=re.I)\n",
    "    s = re.sub(r\"^analy[sz]ing\\b\", \"analysis of\",    s, flags=re.I)\n",
    "    s = re.sub(r\"^investigating\\b\",\"investigation of\", s, flags=re.I)\n",
    "    s = s.strip(\" ,\")\n",
    "    return latex_escape(s or \"the study topic\")\n",
    "\n",
    "def inject_table_into_results_tex(paper: str, table_tex: str) -> str:\n",
    "    mlabel = re.search(r\"\\\\label\\{([^}]+)\\}\", table_tex)\n",
    "    label  = mlabel.group(1) if mlabel else \"tab:data\"\n",
    "    between = re.search(r\"\\\\midrule(.*?)\\\\bottomrule\", table_tex, re.S)\n",
    "    row_count = 0\n",
    "    if between:\n",
    "        row_count = len([ln for ln in between.group(1).splitlines() if \"\\\\\" in ln])\n",
    "    mhdr = re.search(r\"\\\\toprule\\s*(.*?)\\\\\\\\\\s*\\\\midrule\", table_tex, re.S)\n",
    "    header_cols = []\n",
    "    if mhdr:\n",
    "        header_cols = [re.sub(r\"\\s+\", \" \", h.strip()) for h in mhdr.group(1).split(\"&\")]\n",
    "    topic = _topic_phrase_from_context(experiment.get(\"context\",\"\"))\n",
    "    PREVIEW_K = 6\n",
    "    if len(header_cols) <= PREVIEW_K:\n",
    "        cols_preview, tail = \", \".join(header_cols), \"\"\n",
    "    else:\n",
    "        cols_preview = \", \".join(header_cols[:PREVIEW_K])\n",
    "        tail = f\" (+{len(header_cols)-PREVIEW_K} more)\"\n",
    "    templates = [\n",
    "        r\"The dataset summarized in Table~\\ref{%LABEL%} (N=%N%) addresses %TOPIC% and contains the columns: %COLS%%TAIL%.\",\n",
    "        r\"Table~\\ref{%LABEL%} (N=%N%) presents data for %TOPIC% with columns: %COLS%%TAIL%.\",\n",
    "        r\"For %TOPIC%, Table~\\ref{%LABEL%} reports N=%N% observations across the columns: %COLS%%TAIL%.\",\n",
    "    ]\n",
    "    t_idx = (len(header_cols) + row_count + sum(map(ord, label))) % len(templates)\n",
    "    lead_sentence = (templates[t_idx]\n",
    "                     .replace(\"%LABEL%\", label)\n",
    "                     .replace(\"%N%\", str(row_count))\n",
    "                     .replace(\"%TOPIC%\", topic)\n",
    "                     .replace(\"%COLS%\", cols_preview)\n",
    "                     .replace(\"%TAIL%\", tail))\n",
    "    input_names  = list(experiment[\"inputs\"].keys())\n",
    "    output_names = list(experiment[\"outputs\"].keys())\n",
    "    items = []\n",
    "    for i, (name, (unit, dist, params)) in enumerate(experiment[\"inputs\"].items(), 1):\n",
    "        items.append(rf\"\\item $x_{i}$: {_format_header_with_units(name)} sampled as ${_dist_to_tex(dist, params)}$\")\n",
    "    for j, (name, (unit, formula, noise)) in enumerate(experiment[\"outputs\"].items(), 1):\n",
    "        noise_tex = \"\"\n",
    "        if isinstance(noise, tuple) and noise and str(noise[0]).lower() == \"gaussian\":\n",
    "            mu, sd = noise[1]; noise_tex = rf\"\\;+\\; \\mathcal{{N}}({mu},\\,{sd})\"\n",
    "        f_ltx, _, _ = _formula_to_latex(formula, input_names, output_names)\n",
    "        items.append(rf\"\\item $y_{j}$: {_format_header_with_units(name)} via $y_{j} = {f_ltx}{noise_tex}$\")\n",
    "    items_block = \"\\\\begin{itemize}\\n\" + \"\\n\".join(items) + \"\\n\\\\end{itemize}\\n\"\n",
    "    approach = (\n",
    "        \"Inputs $x_1,\\\\ldots,x_m$ are drawn from the specified distributions, and \"\n",
    "        \"responses $y_1,\\\\ldots,y_k$ are computed from the groundâ€‘truth equations \"\n",
    "        \"with optional noise.\"\n",
    "    )\n",
    "    lead = (\n",
    "        \"\\n\\n\"\n",
    "        + lead_sentence + \"\\n \"\n",
    "        + approach + \"\\n\\n\"\n",
    "        + items_block + \"\\n\"\n",
    "    )\n",
    "    pattern = re.compile(r'^[ \\t]*#{1,6}\\s*Results\\b.*$', re.IGNORECASE | re.MULTILINE)\n",
    "    mr = pattern.search(paper)\n",
    "    block = lead + table_tex + \"\\n\"\n",
    "    if mr:\n",
    "        insert_at = mr.end()\n",
    "        return paper[:insert_at] + block + paper[insert_at:]\n",
    "    else:\n",
    "        return paper + \"\\n\\n\" + block\n",
    "\n",
    "def md_to_tex(text: str) -> str:\n",
    "    text = markdown_emphasis_to_tex(text)\n",
    "    out = []\n",
    "    for ln in text.splitlines():\n",
    "        if ln.startswith(\"##### \"): out.append(r\"\\paragraph{\"      + latex_escape(ln[6:]) + \"}\")\n",
    "        elif ln.startswith(\"#### \"): out.append(r\"\\subsubsection{\" + latex_escape(ln[5:]) + \"}\")\n",
    "        elif ln.startswith(\"### \"):  out.append(r\"\\subsubsection{\" + latex_escape(ln[4:]) + \"}\")\n",
    "        elif ln.startswith(\"## \"):   out.append(r\"\\subsection{\"    + latex_escape(ln[3:]) + \"}\")\n",
    "        elif ln.startswith(\"# \"):    out.append(r\"\\section{\"       + latex_escape(ln[2:]) + \"}\")\n",
    "        else:                        out.append(ln)\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def clean_title_for_latex(s: str) -> str:\n",
    "    if not s: return \"\"\n",
    "    s = re.sub(r\"\\$\\$.*?\\$\\$|\\$[^$]*\\$|\\\\\\[.*?\\\\\\]|\\\\\\((?:.|\\n)*?\\\\\\)\", \"\", s, flags=re.S)\n",
    "    for cmd in [\"textbf\",\"emph\",\"textit\",\"textsc\",\"texttt\",\"textsf\",\"mathrm\"]:\n",
    "        s = re.sub(rf\"\\\\{cmd}\\{{(.*?)\\}}\", r\"\\1\", s, flags=re.S)\n",
    "    s = re.sub(r\"\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?\\{(.*?)\\}\", r\"\\1\", s, flags=re.S)\n",
    "    s = re.sub(r\"\\\\[a-zA-Z]+\\*?(?:\\[[^\\]]*\\])?\", \"\", s)\n",
    "    s = re.sub(r\"\\*\\*(.+?)\\*\\*\", r\"\\1\", s, flags=re.S)\n",
    "    s = re.sub(r\"(?<!\\\\)\\*(?!\\s)(.+?)(?<!\\s)\\*\", r\"\\1\", s, flags=re.S)\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def build_tex_doc(title: str, abstract: str, body_tex: str) -> str:\n",
    "    safe_title = latex_escape(clean_title_for_latex(title))\n",
    "    return (\n",
    "        \"\\\\documentclass[11pt]{article}\\n\"\n",
    "        \"\\\\usepackage[a4paper,margin=1in]{geometry}\\n\"\n",
    "        \"\\\\usepackage{booktabs}\\n\"\n",
    "        \"\\\\usepackage{amsmath, amssymb}\\n\"\n",
    "        \"\\\\usepackage{siunitx}\\n\"\n",
    "        \"\\\\usepackage{hyperref}\\n\"\n",
    "        \"\\\\usepackage{caption}\\n\"\n",
    "        \"\\\\captionsetup[table]{position=top}\\n\\n\"\n",
    "        \"\\\\title{\" + safe_title + \"}\\n\"\n",
    "        \"\\\\author{}\\n\"\n",
    "        \"\\\\date{}\\n\\n\"\n",
    "        \"\\\\begin{document}\\n\"\n",
    "        \"\\\\maketitle\\n\\n\"\n",
    "        \"\\\\begin{abstract}\\n\"\n",
    "        + markdown_emphasis_to_tex(strip_meta(abstract)) + \"\\n\"\n",
    "        \"\\\\end{abstract}\\n\\n\"\n",
    "        + body_tex + \"\\n\\n\"\n",
    "        \"\\\\end{document}\\n\"\n",
    "    )\n",
    "\n",
    "def _json_fix_backslashes(s: str) -> str:\n",
    "    return re.sub(r'\\\\(?![\\\\/\"bfnrtu])', r'\\\\\\\\', s)\n",
    "\n",
    "def strip_latex_math(text: str) -> str:\n",
    "    patterns = [\n",
    "        r\"\\$\\$.*?\\$\\$\",\n",
    "        r\"\\$[^$]*\\$\",\n",
    "        r\"\\\\\\[.*?\\\\\\]\",\n",
    "        r\"\\\\\\((?:.|\\n)*?\\\\\\)\",\n",
    "        r\"\\\\begin\\{equation\\*?\\}.*?\\\\end\\{equation\\*?\\}\",\n",
    "        r\"\\\\begin\\{align\\*?\\}.*?\\\\end\\{align\\*?\\}\",\n",
    "        r\"\\\\begin\\{gather\\*?\\}.*?\\\\end\\{gather\\*?\\}\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        text = re.sub(pat, \"\", text, flags=re.S)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "def fix_malformed_environments(tex: str) -> str:\n",
    "    def _clean_env(match, is_begin=True):\n",
    "        env = match.group(1)\n",
    "        env = re.split(r'\\\\', env, 1)[0]\n",
    "        env = re.sub(r'[^A-Za-z*]', '', env)\n",
    "        tag = \"begin\" if is_begin else \"end\"\n",
    "        return fr\"\\{tag}{{{env}}}\"\n",
    "    tex = re.sub(r\"\\\\begin\\{([^}]*)\\}\",  lambda m: _clean_env(m, True),  tex)\n",
    "    tex = re.sub(r\"\\\\end\\{([^}]*)\\}\",    lambda m: _clean_env(m, False), tex)\n",
    "    stack, out = [], []\n",
    "    token_pat = re.compile(r\"\\\\(begin|end)\\{([A-Za-z*]+)\\}\")\n",
    "    idx = 0\n",
    "    for m in token_pat.finditer(tex):\n",
    "        out.append(tex[idx:m.end()])\n",
    "        idx = m.end()\n",
    "        typ, env = m.group(1), m.group(2)\n",
    "        if typ == \"begin\":\n",
    "            stack.append(env)\n",
    "        else:\n",
    "            if stack and stack[-1] == env:\n",
    "                stack.pop()\n",
    "            else:\n",
    "                out[-1] = out[-1].replace(m.group(0), \"\")\n",
    "    out.append(tex[idx:])\n",
    "    for env in reversed(stack):\n",
    "        out.append(f\"\\n\\\\end{{{env}}}\\n\")\n",
    "    return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded2184d",
   "metadata": {},
   "source": [
    "### Latex complier for the verification of the generated Latex code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3adbb0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tex_engine_cmd():\n",
    "    # tries latexmk â†’ pdflatex â†’ lualatex\n",
    "    for cmd in (['latexmk', '-pdf', '-interaction=nonstopmode', '-halt-on-error'],\n",
    "                ['pdflatex', '-interaction=nonstopmode', '-halt-on-error'],\n",
    "                ['lualatex', '-interaction=nonstopmode', '-halt-on-error']):\n",
    "        if shutil.which(cmd[0]):\n",
    "            return cmd\n",
    "    return None\n",
    "\n",
    "def validate_tex_source(tex_source: str, base_name: str, timeout_sec: int = 120):\n",
    "    cmd_base = _tex_engine_cmd()\n",
    "    if not cmd_base:\n",
    "        return None, \"TeX engine not found\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        tex_path = pathlib.Path(tmpdir) / f\"{base_name}.tex\"\n",
    "        tex_path.write_text(tex_source, encoding=\"utf-8\")\n",
    "\n",
    "        proc = subprocess.run(\n",
    "            cmd_base + [tex_path.name],\n",
    "            cwd=tmpdir,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            timeout=timeout_sec)\n",
    "        ok  = (proc.returncode == 0)\n",
    "        log = proc.stdout.decode(\"utf-8\", errors=\"ignore\")\n",
    "        return ok, log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3d9c3",
   "metadata": {},
   "source": [
    "### LLM calls & main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13b81b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_paper(stats_json: str) -> str:\n",
    "    N = experiment[\"N_OBS\"]\n",
    "    # prompt = (\n",
    "    #     f\"{experiment['context']}\\n\\n\"\n",
    "    #     f\"Motivation: {experiment['motivation']}\\n\\n\"\n",
    "    #     f\"{experiment.get('constraints','')}\\n\\n\"\n",
    "    #     f\"Dataset summary statistics (JSON):\\n{stats_json}\\n\\n\"\n",
    "    #     f\"Write an ~{experiment['paper_words']}-word paper with sections:\\n\"\n",
    "    #     \"# Introduction\\n# Methods\\n# Results\\n# Discussion\\n\\n\"\n",
    "    #     \"â€¢ Use LaTeX math ($...$) and LaTeX emphasis (\\\\textbf{}, \\\\emph{}). \"\n",
    "    #     \"Do NOT use Markdown.\\n\"\n",
    "    #     \"â€¢ Base Results on numeric trends; cite â‰¥3 quantitative findings.\\n\"\n",
    "    #     \"â€¢ Do NOT mention dataset generation, scripts, prompts, ground-truth formulas, or noise.\\n\"\n",
    "    # )\n",
    "    prompt = (\n",
    "        f\"{experiment['context']}\\n\\n\"\n",
    "        f\"Motivation: {experiment['motivation']}\\n\\n\"\n",
    "        f\"{experiment.get('constraints','')}\\n\\n\"\n",
    "        f\"Dataset summary statistics (JSON):\\n{stats_json}\\n\\n\"\n",
    "        f\"Write an ~{experiment['paper_words']}-word LaTeX paper with sections:\\n\"\n",
    "        \"# Introduction\\n# Methods\\n# Results\\n# Discussion\\n\\n\"\n",
    "        \"â€¢ Use LaTeX emphasis (\\\\textbf{}, \\\\emph{}). Do NOT use Markdown.\\n\"\n",
    "        \"â€¢ Report quantitative observations (ranges, means/SDs, optional 95% CIs) and the tested condition ranges.\\n\"\n",
    "        \"â€¢ Do NOT introduce or display any equations. Do NOT include an Inputs/Outputs bullet list.\\n\"\n",
    "        \"â€¢ Do NOT mention dataset generation, scripts, prompts, ground-truth formulas, or noise.\\n\"\n",
    "    )\n",
    "    res = client.chat.completions.create(\n",
    "        model=experiment[\"model_id\"],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=experiment[\"tok_paper\"],\n",
    "        temperature=0.4,\n",
    "    )\n",
    "    return strip_meta(res.choices[0].message.content.strip())\n",
    "\n",
    "def llm_title_abs(paper):\n",
    "    prompt = (\n",
    "        \"Provide JSON ONLY with keys 'title' and 'abstract'. \"\n",
    "        \"Use LaTeX emphasis (\\\\textbf{}, \\\\emph{}) not Markdown. \"\n",
    "        \"In JSON string values, ESCAPE EVERY BACKSLASH (e.g., write \\\\\\\\emph{}, not \\\\emph{}). \"\n",
    "        \"Do NOT mention platform constraints/tokens. \"\n",
    "        \"The ABSTRACT MUST BE PROSE ONLY with NO MATH: do not include $...$, $$...$$, \\\\(\\\\), \\\\[\\\\], \"\n",
    "        \"or any equation environments.\\n\\n\"\n",
    "        \"Paper:\\n\" + paper)\n",
    "    res  = client.chat.completions.create(\n",
    "        model=experiment[\"model_id\"],\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        max_tokens=experiment[\"tok_titleabs\"], temperature=0.4)\n",
    "    m = re.search(r\"\\{.*\\}\", res.choices[0].message.content, re.S)\n",
    "    if not m:\n",
    "        raise ValueError(\"Model did not return JSON for title/abstract.\")\n",
    "    block = m.group(0)\n",
    "    fixed = _json_fix_backslashes(block)\n",
    "    try:\n",
    "        meta = json.loads(fixed)\n",
    "    except json.JSONDecodeError:\n",
    "        meta = json.loads(block.replace(\"\\\\\", \"\\\\\\\\\"))\n",
    "    abstract = meta.get(\"abstract\",\"\")\n",
    "    abstract = \"\\n\".join(\n",
    "        ln for ln in abstract.splitlines()\n",
    "        if not ln.lower().lstrip().startswith(\"title:\")\n",
    "    ).strip()\n",
    "    abstract = strip_meta(abstract)\n",
    "    abstract = strip_latex_math(abstract)\n",
    "    abstract = markdown_emphasis_to_tex(abstract)\n",
    "    meta[\"abstract\"] = abstract\n",
    "    candidates = meta.get(\"titles\") if isinstance(meta.get(\"titles\"), list) else [meta.get(\"title\",\"\")]\n",
    "    chosen = choose_diverse_title(candidates, key=paper)\n",
    "    meta[\"title\"] = clean_title_for_latex(chosen)\n",
    "    return meta\n",
    "\n",
    "def _normalize_ws(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "USED_TITLES = set()\n",
    "\n",
    "def llm_alternate_titles(paper, banned):\n",
    "    prompt = (\n",
    "        \"Return JSON ONLY with key 'titles' as an array of 8 distinct candidate titles \"\n",
    "        \"(8â€“14 words each). Titles must be derived from and faithful to the paper content \"\n",
    "        \"provided below. Do not introduce topics that are not inferable from this content. \"\n",
    "        \"Use plain text (no LaTeX/math). \"\n",
    "        \"Avoid any title in this banned list:\\n\"\n",
    "        + json.dumps(sorted(list(banned))) +\n",
    "        \"\\n\\nPaper:\\n\" + paper\n",
    "    )\n",
    "    res = client.chat.completions.create(\n",
    "        model=experiment['model_id'],\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=experiment[\"tok_titleabs\"],\n",
    "        temperature=0.8, presence_penalty=0.6, frequency_penalty=0.3\n",
    "    )\n",
    "    m = re.search(r\"\\{.*\\}\", res.choices[0].message.content, re.S)\n",
    "    if not m:\n",
    "        return []\n",
    "    block = _json_fix_backslashes(m.group(0))\n",
    "    try:\n",
    "        data = json.loads(block)\n",
    "    except json.JSONDecodeError:\n",
    "        data = json.loads(m.group(0).replace(\"\\\\\", \"\\\\\\\\\"))\n",
    "    raw = data.get(\"titles\", [])\n",
    "    seen, out = set(), []\n",
    "    def _tokens(s: str):\n",
    "        return re.findall(r\"[A-Za-z][A-Za-z0-9\\-]{2,}\", (s or \"\").lower())\n",
    "    stop = {\n",
    "        \"the\",\"and\",\"for\",\"with\",\"from\",\"into\",\"that\",\"this\",\"these\",\"those\",\n",
    "        \"study\",\"paper\",\"result\",\"results\",\"method\",\"methods\",\"introduction\",\n",
    "        \"discussion\",\"conclusion\",\"data\",\"dataset\",\"analysis\",\"approach\",\n",
    "        \"using\",\"use\",\"based\",\"model\",\"models\",\"over\",\"under\",\"between\",\"within\",\n",
    "        \"across\",\"about\",\"without\",\"against\",\"toward\",\"towards\",\"through\"\n",
    "    }\n",
    "    vocab = {t for t in _tokens(paper) if t not in stop}\n",
    "    for t in raw:\n",
    "        tt = clean_title_for_latex(_normalize_ws(t))\n",
    "        if not tt:\n",
    "            continue\n",
    "        low = tt.lower()\n",
    "        if low in seen or tt in banned:\n",
    "            continue\n",
    "        tt_tokens = [w for w in _tokens(tt) if w not in stop]\n",
    "        if not tt_tokens:\n",
    "            continue\n",
    "        overlap = sum(1 for w in tt_tokens if w in vocab)\n",
    "        if overlap >= max(3, len(tt_tokens) // 3):\n",
    "            seen.add(low)\n",
    "            out.append(tt)\n",
    "    return out\n",
    "\n",
    "def choose_diverse_title(candidates, key: str) -> str:\n",
    "    uniq, seen = [], set()\n",
    "    for t in candidates or []:\n",
    "        tt = _normalize_ws(t)\n",
    "        if not tt: continue\n",
    "        low = tt.lower()\n",
    "        if low in seen: continue\n",
    "        seen.add(low); uniq.append(tt)\n",
    "    if not uniq:\n",
    "        return \"Untitled\"\n",
    "    def score(t):\n",
    "        n = len(t.split())\n",
    "        return -abs(n - 11)\n",
    "    uniq.sort(key=score, reverse=True)\n",
    "    h = int(hashlib.sha256(key.encode(\"utf-8\")).hexdigest(), 16)\n",
    "    return uniq[h % len(uniq)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850ac53",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff009fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_tex(tex: str) -> str:\n",
    "    # Fix siunitx percent\n",
    "    tex = tex.replace(r'\\si{%}', r'\\si{\\percent}')\n",
    "    # Fix doubled backslashes from LLM: \\\\textbf, \\\\emph, \\\\si, \\\\num, \\\\qty\n",
    "    tex = re.sub(r'\\\\{2}(textbf|emph|si|SI|num|qty)\\b', r'\\\\\\1', tex)\n",
    "    # Nicer header label: \"q e\" -> \"$q_e$\" (surrounded by non-word boundaries)\n",
    "    tex = re.sub(r'(\\W)q\\s+e(\\W)', r'\\1$q_e$\\2', tex)\n",
    "    # Escape bare % outside \\si{...}\n",
    "    tex = re.sub(r'(?<!\\\\)%', r'\\\\%', tex)\n",
    "    tex = re.sub(\n",
    "        r'The dataset summarized in Table~\\\\ref\\{[^}]+\\}.*?\\\\begin\\{table\\}',\n",
    "        r'\\\\begin{table}',\n",
    "        tex,\n",
    "        flags=re.S\n",
    "    )\n",
    "    # 2) Kill any line that contains np.*** or code-like noise in prose\n",
    "    tex = re.sub(r'^.*\\bnp\\.(minimum|maximum|clip)\\b.*$\\n?', '', tex, flags=re.M)\n",
    "    # (optional) also drop Uniform(â€¦), \\mathcal{N}(â€¦) recipe lines if they sneak in\n",
    "    tex = re.sub(r'^.*\\bUniform\\(|\\\\mathcal\\{N\\}\\s*\\(.*$\\n?', '', tex, flags=re.M)\n",
    "\n",
    "    # 3) Remove the stray outline/toc block some runs append at the very end\n",
    "    tex = re.sub(r'\\n\\s*Introduction\\s+Methods.*?Conclusion\\s*$', '', tex, flags=re.S)\n",
    "    # collapse erroneous linebreak-before-percent\n",
    "    tex = tex.replace('\\\\\\\\%', r'\\%')\n",
    "    return tex\n",
    "\n",
    "def normalize_sections(tex: str) -> str:\n",
    "    # Fix the common broken patterns\n",
    "    tex = tex.replace(r'\\section\\emph{{', r'\\section{')\n",
    "    tex = tex.replace(r'\\section}{',    r'\\section{')\n",
    "    tex = tex.replace(r'\\section\\emph{', r'\\section{')\n",
    "    tex = tex.replace(r'\\section{{',     r'\\section{')\n",
    "\n",
    "    # Canonicalize any \\section line that contains one of the standard titles\n",
    "    titles = [\"Introduction\", \"Methods\", \"Results\", \"Discussion\"]\n",
    "    for t in titles:\n",
    "        tex = re.sub(\n",
    "            rf'(?m)^\\s*\\\\section[^\\n]*{re.escape(t)}[^\\n]*$',\n",
    "            rf'\\\\section{{{t}}}',\n",
    "            tex\n",
    "        )\n",
    "    return tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d25fd20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–¶ Dataset 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7838/1015078613.py:73: DeprecationWarning: 'maxsplit' is passed as positional argument\n",
      "  s = re.split(r'[.!?\\n]', s, 1)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âš  validation skipped (no TeX engine); saved paper01.tex\n",
      "ðŸŽ‰  Done. Files in: /home/hari/aros/verification/Output/Phosphate_Songnen_run\n"
     ]
    }
   ],
   "source": [
    "MAX_RETRIES = 2   # extra attempts after the first failure\n",
    "\n",
    "for i in range(1, experiment[\"NUM_DATASETS\"] + 1):\n",
    "    print(f\"â–¶ Dataset {i}/{experiment['NUM_DATASETS']}\")\n",
    "\n",
    "    df        = add_outputs(sample_inputs(experiment[\"N_OBS\"]))\n",
    "    stats     = stats_json(df)\n",
    "\n",
    "    attempt   = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        paper_md = llm_paper(stats)\n",
    "        label    = f\"tab:data{i:02d}\"\n",
    "        table_tex = df_to_latex_table(df, caption=\"Generated dataset\", label=label)\n",
    "        paper_md_with_table = inject_table_into_results_tex(paper_md, table_tex)\n",
    "        body_tex = md_to_tex(paper_md_with_table)\n",
    "\n",
    "        meta      = llm_title_abs(paper_md_with_table)\n",
    "        alts      = llm_alternate_titles(paper_md_with_table + \"\\n\\nABSTRACT:\\n\" + meta.get(\"abstract\", \"\"), USED_TITLES)\n",
    "        candidates = [t for t in alts if t and t not in USED_TITLES] or \\\n",
    "                     (meta.get(\"titles\") if isinstance(meta.get(\"titles\"), list) else [meta.get(\"title\", \"\")])\n",
    "        title      = choose_diverse_title(candidates, key=paper_md_with_table)\n",
    "        if not title or title in USED_TITLES:\n",
    "            base  = clean_title_for_latex(meta.get(\"title\", \"\")) or \"Untitled\"\n",
    "            title = base if base not in USED_TITLES else f\"{base} â€” Study {i:02d}\"\n",
    "        USED_TITLES.add(title)\n",
    "        meta[\"title\"] = title\n",
    "\n",
    "        tex_doc = build_tex_doc(meta[\"title\"], meta[\"abstract\"], body_tex)\n",
    "        tex_doc = sanitize_tex(tex_doc) \n",
    "        tex_doc = normalize_sections(tex_doc) \n",
    "        ok, log = validate_tex_source(tex_doc, f\"paper{i:02d}\")\n",
    "        if ok is None:\n",
    "            (OUT / f\"paper{i:02d}.tex\").write_text(tex_doc, encoding=\"utf-8\")\n",
    "            print(\"   âš  validation skipped (no TeX engine); saved\", f\"paper{i:02d}.tex\")\n",
    "            break\n",
    "\n",
    "        if ok:\n",
    "            (OUT / f\"paper{i:02d}.tex\").write_text(tex_doc, encoding=\"utf-8\")\n",
    "            print(\"   âœ” compiled & saved\", f\"paper{i:02d}.tex\")\n",
    "            break\n",
    "\n",
    "        print(f\"   âœ– compile failed (attempt {attempt}/{MAX_RETRIES + 1})\")\n",
    "        if attempt > MAX_RETRIES:\n",
    "            (OUT / f\"paper{i:02d}.tex\").write_text(tex_doc, encoding=\"utf-8\")\n",
    "            log_dir = OUT / \"logs\"; log_dir.mkdir(exist_ok=True)\n",
    "            (log_dir / f\"paper{i:02d}_compile.log\").write_text(log, encoding=\"utf-8\")\n",
    "            print(\"     saved .tex and compile log to\", f\"logs/paper{i:02d}_compile.log\")\n",
    "            break\n",
    "\n",
    "print(\"ðŸŽ‰  Done. Files in:\", OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec014f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
